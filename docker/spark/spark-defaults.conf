# Spark Configuration for Ecommerce Analytics

# Application Settings
spark.app.name=EcommerceRealtimeAnalytics
spark.master=spark://spark-master:7077

# Driver Settings
spark.driver.memory=1g
spark.driver.maxResultSize=512m
spark.driver.cores=1

# Executor Settings
spark.executor.memory=2g
spark.executor.cores=2
spark.executor.instances=2
spark.executor.memoryFraction=0.8

# Serialization
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false

# SQL Settings
spark.sql.adaptive.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
spark.sql.adaptive.advisory.partitionSizeInBytes=64MB
spark.sql.execution.arrow.pyspark.enabled=true
spark.sql.execution.arrow.maxRecordsPerBatch=10000

# Streaming Settings
spark.streaming.backpressure.enabled=true
spark.streaming.receiver.maxRate=1000
spark.streaming.kafka.maxRatePerPartition=1000
spark.sql.streaming.checkpointLocation=/tmp/spark-checkpoint

# Network Settings
spark.network.timeout=800s
spark.sql.broadcastTimeout=36000

# Storage Settings
spark.storage.level=MEMORY_AND_DISK_SER
spark.rdd.compress=true

# Shuffle Settings
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.shuffle.service.enabled=true

# Dynamic Allocation
spark.dynamicAllocation.enabled=false
spark.dynamicAllocation.minExecutors=1
spark.dynamicAllocation.maxExecutors=4

# Event Log
spark.eventLog.enabled=true
spark.eventLog.dir=/tmp/spark-events
spark.history.fs.logDirectory=/tmp/spark-events

# UI Settings
spark.ui.port=4040
spark.ui.retainedJobs=1000
spark.ui.retainedStages=1000

# Metrics
spark.metrics.conf.*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink
spark.metrics.conf.*.sink.console.period=10
spark.metrics.conf.*.sink.console.unit=seconds

# Security (for production, enable authentication)
spark.authenticate=false
spark.network.crypto.enabled=false
spark.io.encryption.enabled=false

# Kubernetes/Docker specific
spark.kubernetes.container.image.pullPolicy=Always
spark.kubernetes.executor.deleteOnTermination=true

# Catalog Settings
spark.sql.catalog.spark_catalog=org.apache.spark.sql.hive.HiveSessionCatalog
spark.sql.warehouse.dir=/tmp/spark-warehouse

# External Systems
# Kafka
spark.kafka.bootstrap.servers=kafka:29092
spark.kafka.security.protocol=PLAINTEXT

# MySQL
spark.sql.jdbc.driver.class=com.mysql.cj.jdbc.Driver
spark.sql.execution.datasources.jdbc.pushDownPredicate=true
spark.sql.execution.datasources.jdbc.pushDownAggregate=true